{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# panel、wes数据分析\n",
    "GATK4.0为了追求效率，不如3.8的准确性高；GATK速度慢并且准的原因是因为有一步local assembly，因此只对SNP感兴趣的话，我个人认为可以不用GATK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用fastq-mcf去接头\n",
    "fastq-mcf -l 50 -o $outdir/$sample\\_R1.fastq.gz -o $outdir/$sample\\_R2.fastq.gz $adapter $raw_data1 $raw_data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 比对到基因组\n",
    "由bwa生成的sam文件是按字典式排序法进行的排序（chr10,chr11..chr19,chr1,chr20..chr22,chr2,chr3..chrM,chrX,chrY),但gatk在进行call snp的时候是按照染色体组型进行的（chrM,chr1,chr2..chr22,chrX,chrY),因此要对原始sam文件进行reorder; \n",
    "(bwa mem -t $proc -M $bwa_index -R \"@RG\\tID:$sample\\tSM:$sample\\tPL:Illumina\" $inRead1 $inRead2 || echo -n 'error') | sentieon util sort -o $outdir/$sample.sort.bam -t $proc --sam2bam -i - \n",
    "-R参数对文件进行加头处理，因为gatk2.0以上版本不再支持无头文件的变异检测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 去PCR重复\n",
    "PCR重复：如果两条reads具有相同的长度而且比对到基因组的同一位置，那么就认为这样的reads是由PCR扩增而来，就会被标记；\n",
    "在制备文库的过程中，由于PCR扩增过程中会存在一些偏差，也就是说有的序列会被过量扩增。这样，在比对的时候，这些过量扩增出来的完全相同的序列就会比对到基因组的相同位置。而这些过量扩增的reads并不是基因组自身固有序列，不能作为变异检测的证据，因此，要尽量去除这些由PCR扩增所形成的duplicates\n",
    "\n",
    "sentieon driver -t $proc -i $outdir/$sample.sort.bam --algo LocusCollector --fun score_info $outdir/score.txt\n",
    "sentieon driver -t $proc -i $outdir/$sample.sort.bam --algo Dedup --rmdup --score_info $outdir/score.txt --metrics $outdir/$sample.metrics $outdir/$sample.sort.mkdup.bam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 统计测序质量\n",
    "sentieon driver -r $ref_genome -i $outdir/$sample.sort.mkdup.bam --interval $panel \\\n",
    "         --algo CoverageMetrics --gene_list $ref_annotation/refseq.sorted.txt \\\n",
    "         --cov_thresh 1 --cov_thresh 10 --cov_thresh 20 $outdir/$sample.depth\n",
    "metrics文件中的PERCENT_DUPLICATION是指PCR重复吗？        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## indel重排序\n",
    "sentieon driver -t $proc -r $ref_genome -i 2_mapping/$sample.sort.mkdup.bam --algo Realigner -k $ref_1000G_indel -k $ref_1000M_indel --interval_list $panel $outdir/$sample.realn.bam \n",
    "realignment将比对到indel附近的reads进行局部重新比对，将比对的错误率降到最低。一般来说，绝大部分需要进行重新比对的基因组区域，都是因为插入/缺失的存在，因为在indel附近的比对会出现大量的碱基错配，这些碱基的错配很容易被误认为SNP。还有，在比对过程中，比对算法对于每一条read的处理都是独立的，不可能同时把多条reads与参考基因组比对来排错。因此，即使有一些 reads能够正确的比对到indel，但那些恰恰比对到indel开始或者结束为止的read也会有很高的比对错误率，这都是需要重新比对的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 碱基BQSR \n",
    "对bam文件里reads的碱基质量值进行重新校正，使最后输出的bam文件中reads中碱基的质量值能够更加接近真实值？在reads碱基质量值被校正之前，我们要保留质量值在Q25以上的碱基，但是实际上质量值在Q25的这些碱基的错误率在1%，也就是说质量值只有Q20，这样就会对后续的变异检测的可信度造成影响。还有，在边合成边测序的测序过程中，在reads末端碱基的错误率往往要比起始部位更高。另外，AC的质量值往往要低于TG。BQSR就是要对这些质量值进行校正。\n",
    "根据一些known sites，生成一个校正质量值所需要的数据文件，利用这个文件来生成校正后的数据文件，最后生成碱基质量值校正前后的比较图，利用工具将经过质量值校正的数据输出到新的bam文件中，用于后续的变异检测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 变异检测\n",
    "用Haplotyper call snp，输出gvcf；使用贝叶斯最大似然模型，估计基因型和基因频率，给出后验概率\n",
    "sentieon driver -t $proc -r $ref_genome --interval $panel -i $outdir/$sample.realn.bam -q $outdir/recal_data.table --algo Haplotyper -d $ref_snp --emit_mode gvcf $outdir/gvcf/$sample.g.vcf  \n",
    "--interval 指定对基因组的某一区域进行变异检测； \n",
    "gvcf文件与vcf文件都是vcf文件，不同的是gvcf文件同时会记录为蜕变的微店的覆盖情况，在多个样本的vcf文件进行合并的时候，需要区分./.和0/0的情况，./.是未检出的基因型，而0/0时未突变的基因型，如果仅使用普通的vcf文件进行合并，那么就无法区分出这两种情况，进而对合并结果产生偏差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 过滤变异\n",
    "hardfilt或vqsr\n",
    "vqsr：使用高斯模型，根据已有的真实变异位点（HapMap，omni...）来训练，最后得到一个训练好的能够评估真伪的错误评估模型，评估每一个变异位点发生错误的概率，给出一个得分，就是在训练好的混合高斯模型下，一个位点是真实的概率比上这个位点是假阳性的概率的log值，这个值越大就越好；这个模型首先要拿到真实变异数据集和待过滤的原始变异数据集的交集，然后对这些变异相对于具体注释信息的分布情况进行模拟，将这些变异位点进行聚类，最后根据聚类结果赋予所有变异位点相应的log值，越接近聚类核心的变异位点得到的值越高。设置阈值，log值高于阈值被认为是可信的，低于阈值的就会被过滤掉，设置阈值要兼顾敏感度和质量值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## indel归一化\n",
    "如果不进行归一化，相同的变异可能会因为形式的不同而导致无法比较，例如：\n",
    "    #CHROM    POS    ID    REF    ALT\n",
    "    1         900010 .     GC     GCC\n",
    "    1         900010 .     G      GC\n",
    "这两个位点其实是相同的位点，如果不进行处理，对后续的注释和分析会带来很大的麻烦（但是怎么确定是G后面插入了一个C，不是C后面插入了一个C？）\n",
    "\n",
    "indel的左对齐\n",
    "    #CHROM    POS        ID    REF    ALT\n",
    "    1         900010     .     GC     GCC\n",
    "    1         12255506   .     TCCCCC TCCC\n",
    "通过左对齐后可以变成\n",
    "    #CHROM    POS        ID    REF    ALT\n",
    "    1         900010     .     G      GC \n",
    "    1         12255506   .     TCC    T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注释变异\n",
    "使用vep、annovar和CADD进行注释"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
